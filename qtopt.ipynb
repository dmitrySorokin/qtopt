{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "torch.Size([1, 16, 64, 64])\n",
      "<built-in method type of Tensor object at 0x7f10ec2b4048>\n",
      "cuda\n",
      "uint8 (16, 64, 64)\n",
      "SubprocVecEnv worker: got KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_interf\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "from baselines.common.vec_env.shmem_vec_env import ShmemVecEnv\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.common.vec_env import VecEnvWrapper\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "N_ENVS = 1\n",
    "N_STEPS = 128\n",
    "\n",
    "\n",
    "class StateWrapper(gym.Wrapper):\n",
    "    def __init__(self, e):\n",
    "        super().__init__(e)\n",
    "        self.prev_dist = None\n",
    "        self.prev_angle = None\n",
    "        \n",
    "    def set_noise(self):\n",
    "        rnd = np.random.rand()\n",
    "        self.env.add_noise(rnd * 20)\n",
    "        \n",
    "    def _permutate(self, state):\n",
    "        start = np.random.randint(0, state.shape[0])\n",
    "        result = []\n",
    "        for i in range(start, start + state.shape[0]):\n",
    "            result.append(state[i % state.shape[0]])\n",
    "        return np.asarray(result)\n",
    "    \n",
    "    def _fft(self, state):\n",
    "        state = np.fft.fftn(state)\n",
    "        return np.concatenate([np.real(state), np.imag(state)], axis=0)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.set_noise()\n",
    "        \n",
    "        self.env.x_min = -np.random.uniform(low=0.8, high=3)#-3.57 / 2\n",
    "        self.env.x_max = np.random.uniform(low=0.8, high=3)#3.57 / 2\n",
    "\n",
    "        state = self.env.reset(**kwargs)\n",
    "        state = self._permutate(state)\n",
    "        #state = self._fft(state)\n",
    "        \n",
    "        self.prev_dist = self.env.dist\n",
    "        self.prev_angle = self.env.angle\n",
    "        return (\n",
    "            state, np.array([       \n",
    "                -self.env.mirror1_screw_x,\n",
    "                -self.env.mirror1_screw_y,\n",
    "                -self.env.mirror2_screw_x,\n",
    "                -self.env.mirror2_screw_y,\n",
    "            ]), \n",
    "            1.0 - self.env.visib\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.set_noise()\n",
    "            \n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        state = self._permutate(state)\n",
    "        #state = self._fft(state)\n",
    "        \n",
    "        delta_dist = self.prev_dist - self.env.dist\n",
    "        delta_angle = self.prev_angle - self.env.angle\n",
    "        \n",
    "        self.prev_dist = self.env.dist\n",
    "        self.prev_angle = self.env.angle\n",
    "                \n",
    "        #reward += delta_dist + delta_angle * 1000\n",
    "        \n",
    "        #reward -= 1.0 / 200.0\n",
    "        \n",
    "        return [\n",
    "            state, np.array([       \n",
    "                -self.env.mirror1_screw_x,\n",
    "                -self.env.mirror1_screw_y,\n",
    "                -self.env.mirror2_screw_x,\n",
    "                -self.env.mirror2_screw_y,\n",
    "            ]), \n",
    "            1.0 - self.env.visib\n",
    "        ], reward, done, info\n",
    "    \n",
    "\n",
    "class MyVecPyTorch(VecEnvWrapper):\n",
    "    def __init__(self, venv, device):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MyVecPyTorch, self).__init__(venv)\n",
    "        self.device = device\n",
    "        # TODO: Fix data types\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        imgs = np.stack(obs[:,0])\n",
    "        actions = np.stack(obs[:,1])\n",
    "        visibs = np.stack(obs[:,2])\n",
    "        obs = (\n",
    "            torch.from_numpy(imgs).float().to(self.device),\n",
    "            torch.from_numpy(actions).to(self.device),\n",
    "            torch.from_numpy(visibs).to(self.device)\n",
    "        )\n",
    "        return obs\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        if isinstance(actions, torch.LongTensor):\n",
    "            # Squeeze the dimension for discrete actions\n",
    "            actions = actions.squeeze(1)\n",
    "        actions = actions.cpu().numpy()\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, reward, done, info = self.venv.step_wait()\n",
    "        imgs = np.stack(obs[:,0])\n",
    "        actions = np.stack(obs[:,1])\n",
    "        visibs = np.stack(obs[:,2])\n",
    "        obs = (\n",
    "            torch.from_numpy(imgs).float().to(self.device),\n",
    "            torch.from_numpy(actions).to(self.device),\n",
    "            torch.from_numpy(visibs).to(self.device)\n",
    "        )\n",
    "        reward = torch.from_numpy(reward).unsqueeze(dim=1).float()\n",
    "        return obs, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_interf_env(seed):\n",
    "    env = StateWrapper(gym.make('interf-v1'))\n",
    "    env.set_calc_reward('visib_minus_1')\n",
    "    #env.set_calc_reward('delta_visib')\n",
    "    #env.set_calc_image('gpu')\n",
    "    env.seed(seed)\n",
    "    return env\n",
    "\n",
    "\n",
    "temp_env = make_interf_env(0)\n",
    "obs = temp_env.reset()\n",
    "ACTION_SPACE = temp_env.action_space\n",
    "print(ACTION_SPACE)\n",
    "OBS_SHAPE = obs[0].shape\n",
    "N_ACTIONS = ACTION_SPACE.shape[0]\n",
    "\n",
    "\n",
    "env_lambda = [\n",
    "    lambda env_seed=env_seed: make_interf_env(seed=env_seed)\n",
    "    for env_seed in range(N_ENVS)]\n",
    "\n",
    "envs = SubprocVecEnv(env_lambda, context='fork')\n",
    "#envs = ShmemVecEnv(env_lambda, context='fork')\n",
    "envs = MyVecPyTorch(envs, DEVICE)\n",
    "\n",
    "print(envs.reset()[0].shape)\n",
    "print(envs.reset()[0].type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(DEVICE)\n",
    "print(obs[0].dtype, obs[0].shape)\n",
    "#assert obs.shape == (16, 64, 64), obs.shape\n",
    "#assert obs.dtype == torch.uint8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "GPU = True\n",
    "device_idx = 0\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, action_target, value_target):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done, action_target, value_target)\n",
    "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        choices = np.array(self.buffer)\n",
    "        idx = random.choice(len(choices), batch_size)\n",
    "        batch = choices[idx]\n",
    "        state, action, reward, next_state, done, action_target, value_target = map(np.stack, zip(*batch)) # stack for each element\n",
    "        state = state.reshape(-1, 16, 64, 64)\n",
    "        next_state = next_state.reshape(-1, 16, 64, 64)\n",
    "        ''' \n",
    "        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
    "        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
    "        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
    "        np.stack((1,2)) => array([1, 2])\n",
    "        '''\n",
    "        return state, action, reward, next_state, done, action_target, value_target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(object):\n",
    "    \"\"\" cross-entropy method, as optimization of the action policy \"\"\"\n",
    "    def __init__(self, theta_dim, ini_mean_scale=0.0, ini_std_scale=1.0):\n",
    "        self.theta_dim = theta_dim\n",
    "        self.initialize(ini_mean_scale=ini_mean_scale, ini_std_scale=ini_std_scale)\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def initialize(self, ini_mean_scale=0.0, ini_std_scale=0.33):\n",
    "        self.mean = ini_mean_scale * np.ones(self.theta_dim)\n",
    "        self.std = ini_std_scale * np.ones(self.theta_dim)\n",
    "        \n",
    "    def sample(self):\n",
    "        # theta = self.mean + np.random.randn(self.theta_dim) * self.std\n",
    "        theta = self.mean + np.random.normal(size=self.theta_dim) * self.std\n",
    "        return theta\n",
    "\n",
    "    def sample_multi(self, n):\n",
    "        theta_list = []\n",
    "        for i in range(n):\n",
    "            theta_list.append(self.sample())\n",
    "        return np.array(theta_list)\n",
    "\n",
    "    def update(self, selected_samples):\n",
    "        self.mean = np.mean(selected_samples, axis=0)\n",
    "        self.std = np.std(selected_samples, axis=0)  # plus the entropy offset, or else easily get 0 std\n",
    "\n",
    "        return self.mean, self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size=512):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        init_ = lambda m: init(\n",
    "            m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "            constant_(x, 0), nn.init.calculate_gain('relu')\n",
    "        )\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            init_(nn.Conv2d(num_inputs, 32, 8, stride=4)), nn.ReLU(),\n",
    "            init_(nn.Conv2d(32, 64, 4, stride=2)), nn.ReLU(),\n",
    "            init_(nn.Conv2d(64, 32, 3, stride=1)), nn.ReLU(), Flatten(),\n",
    "            init_(nn.Linear(32 * 4 * 4, hidden_size)), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Sequential(\n",
    "            init_(nn.Linear(hidden_size + num_outputs, 512)), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        init_ = lambda m: init(\n",
    "            m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "            constant_(x, 0)\n",
    "        )\n",
    "        \n",
    "        self.linear2 = init_(nn.Linear(512, 1))\n",
    "        \n",
    "    def forward(self, inputs, action):\n",
    "        inputs = inputs.float()\n",
    "        x = inputs / torch.mean(inputs, dim=(2, 3), keepdim=True)\n",
    "        x = self.main(inputs)\n",
    "\n",
    "        x = torch.cat([x, action], 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QT_Opt():\n",
    "    def __init__(self, replay_buffer, obs_shape, action_space, q_lr=3e-4, cem_update_itr=2, select_num=6, num_samples=64):\n",
    "        self.num_samples = num_samples\n",
    "        self.select_num = select_num\n",
    "        self.cem_update_itr = cem_update_itr\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        num_inputs = obs_shape[0]\n",
    "        num_outputs = action_space.shape[0]\n",
    "\n",
    "        self.qnet = QNetwork(num_inputs, num_outputs).to(device) # gpu\n",
    "        self.target_qnet1 = QNetwork(num_inputs, num_outputs).to(device)\n",
    "        self.target_qnet2 = QNetwork(num_inputs, num_outputs).to(device)\n",
    "\n",
    "        self.cem = CEM(theta_dim=num_outputs)  # cross-entropy method for updating\n",
    "\n",
    "        self.q_optimizer = optim.Adam(self.qnet.parameters(), lr=q_lr)\n",
    "        self.step_cnt = 0\n",
    "\n",
    "    def update(self, batch_size, gamma=0.9, soft_tau=1e-4, update_delay=10000):\n",
    "        state, action, reward, next_state, done, action_target, value_target = self.replay_buffer.sample(batch_size)\n",
    "        #print('action_target', action_target.shape, 'value_target', value_target.shape)\n",
    "        self.step_cnt += 1\n",
    "\n",
    "        state = torch.from_numpy(state).to(device)\n",
    "        next_state_ = torch.from_numpy(next_state).to(device)\n",
    "        action = torch.FloatTensor(action).to(device)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1).to(device)  # reward is single value, unsqueeze() to add one dim to be [reward] at the sample dim;\n",
    "        done = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        predict_q = self.qnet(state, action) # predicted Q(s,a) value\n",
    "\n",
    "        # get argmax_a' from the CEM for the target Q(s', a')\n",
    "        new_next_action = []\n",
    "        for i in range(batch_size):      # batch of states, use them one by one, to prevent the lack of memory\n",
    "            new_next_action.append(self.cem_optimal_action(next_state[i].reshape(-1, 16, 64, 64)))\n",
    "        new_next_action = torch.FloatTensor(new_next_action).to(device)\n",
    "\n",
    "        target_q_min = torch.min(\n",
    "            self.target_qnet1(next_state_, new_next_action),\n",
    "            self.target_qnet2(next_state_, new_next_action)\n",
    "        )\n",
    "\n",
    "        target_q = reward + (1 - done) * gamma * target_q_min\n",
    "\n",
    "        \n",
    "        # MSE loss, note that original paper uses cross-entropy loss\n",
    "        q_loss = ((predict_q - target_q.detach())**2).mean()\n",
    "        \n",
    "        ######################\n",
    "        #value_target = torch.from_numpy(value_target).float().to(device)\n",
    "        #my_loss = ((predict_q - value_target)**2).mean()\n",
    "        #q_loss += my_loss\n",
    "        #####################\n",
    "\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # update the target nets, according to original paper:\n",
    "        # one with Polyak averaging, another with lagged/delayed update\n",
    "        self.target_qnet1 = self.target_soft_update(self.qnet, self.target_qnet1, soft_tau)\n",
    "        self.target_qnet2 = self.target_delayed_update(self.qnet, self.target_qnet2, update_delay)\n",
    "        \n",
    "        return q_loss.detach().cpu().numpy()\n",
    "\n",
    "    def cem_optimal_action(self, state):\n",
    "        \"\"\" evaluate action wrt Q(s,a) to select the optimal using CEM \"\"\"\n",
    "        cuda_states = torch.from_numpy(np.vstack([state] * self.num_samples)).to(device)\n",
    "        # every time use a new cem, cem is only for deriving the argmax_a'\n",
    "        self.cem.initialize()\n",
    "        for itr in range(self.cem_update_itr):\n",
    "            actions = self.cem.sample_multi(self.num_samples)\n",
    "            q_values = self.target_qnet1(\n",
    "                cuda_states,\n",
    "                torch.from_numpy(actions).float().to(device)\n",
    "            ).detach().cpu().numpy().reshape(-1) # 2 dim to 1 dim\n",
    "            max_idx = q_values.argsort()[-1]  # select one maximal q\n",
    "            idx = q_values.argsort()[-int(self.select_num):]  # select top maximum q\n",
    "            selected_actions = actions[idx]\n",
    "            _, _ = self.cem.update(selected_actions)\n",
    "        optimal_action = actions[max_idx]\n",
    "        return optimal_action\n",
    "\n",
    "    def target_soft_update(self, net, target_net, soft_tau):\n",
    "        \"\"\" Soft update the target net \"\"\"\n",
    "        for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(  # copy data value into target parameters\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "        return target_net\n",
    "\n",
    "    def target_delayed_update(self, net, target_net, update_delay):\n",
    "        \"\"\" delayed update the target net \"\"\"\n",
    "        if self.step_cnt % update_delay == 0:\n",
    "            for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "                target_param.data.copy_(  # copy data value into target parameters\n",
    "                    param.data \n",
    "                )\n",
    "\n",
    "        return target_net\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "        torch.save(self.target_qnet1.state_dict(), path)\n",
    "        torch.save(self.target_qnet2.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.qnet.load_state_dict(torch.load(path))\n",
    "        self.target_qnet1.load_state_dict(torch.load(path))\n",
    "        self.target_qnet2.load_state_dict(torch.load(path))\n",
    "        self.qnet.eval()\n",
    "        self.target_qnet1.eval()\n",
    "        self.target_qnet2.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 64, 64)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 30000\n",
    "\n",
    "print(OBS_SHAPE)\n",
    "print(ACTION_SPACE)\n",
    "\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "qt_opt = QT_Opt(replay_buffer, OBS_SHAPE, ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from collections import deque\n",
    "\n",
    "def evaluate_(model, env, n_games=10):\n",
    "    n_solved = 0\n",
    "    n_steps = 0\n",
    "    dist = 0\n",
    "    angle = 0\n",
    "    rewards = 0\n",
    "    actions = np.zeros(ACTION_SPACE.shape[0])\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        s = env.reset()\n",
    "        visib = env.env.visib\n",
    "        istep = 0\n",
    "        while(True):\n",
    "            s = s[0]\n",
    "            s = torch.FloatTensor(s.reshape(1, 16, 64, 64)).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                action = model.cem_optimal_action(s.cpu())\n",
    "            \n",
    "            actions += action\n",
    "            s, reward, done, info = env.step(action)\n",
    "\n",
    "            istep += 1\n",
    "            if done:\n",
    "                rewards += env.env.visib - visib\n",
    "                n_solved += istep < 200\n",
    "                n_steps += istep\n",
    "                dist += info['dist']\n",
    "                angle += info['angle_between_beams']\n",
    "\n",
    "                break\n",
    "    return (\n",
    "        n_solved / n_games, \n",
    "        n_steps / n_games, \n",
    "        dist / n_games, \n",
    "        angle / n_games, \n",
    "        rewards / n_games, \n",
    "        actions / n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_(qt_opt, make_interf_env(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:01<34:36:47,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  | Reward:  -0.0021778969000028053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/100000 [00:02<34:21:26,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100  | Reward:  0.004551178155628139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/100000 [00:03<34:19:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200  | Reward:  -0.0004161117232501236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/100000 [00:04<33:50:05,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300  | Reward:  -0.0011419849512813267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/100000 [00:06<33:41:36,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400  | Reward:  0.009610435826826073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 6/100000 [00:07<33:38:41,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500  | Reward:  0.004200497312558733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 7/100000 [00:08<33:30:25,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600  | Reward:  0.10879770477992683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 8/100000 [00:09<33:44:13,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700  | Reward:  -0.0007225370607431053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 9/100000 [00:10<33:32:35,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800  | Reward:  -0.0017728522999830281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 10/100000 [00:12<33:17:21,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900  | Reward:  -0.000756861004572094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 11/100000 [00:13<33:34:23,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000  | Reward:  -0.0002510094177109547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 12/100000 [00:14<33:34:29,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1100  | Reward:  0.006658116435754642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 13/100000 [00:15<33:33:41,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1200  | Reward:  -0.0005079181717002845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 14/100000 [00:16<33:16:52,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1300  | Reward:  -0.001342895498835896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitry/dev/gym_interf/gym_interf/envs/interf_env.py:326: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  band_width_y = InterfEnv.lamb / abs(wave_vector2[1])\n",
      "  0%|          | 15/100000 [00:18<33:08:44,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1400  | Reward:  0.00014578103318756315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 16/100000 [00:19<33:14:57,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1500  | Reward:  0.004392929262466543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 17/100000 [00:20<33:08:19,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1600  | Reward:  -7.51900988206352e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 18/100000 [00:21<32:45:46,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1700  | Reward:  -0.0017531859058760942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 19/100000 [00:22<32:44:26,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1800  | Reward:  -0.0021456063793642557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 20/100000 [00:23<32:55:46,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1900  | Reward:  0.005590466865267746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 21/100000 [00:25<32:38:12,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000  | Reward:  -0.00020899750795798827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 22/100000 [00:26<32:48:45,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2100  | Reward:  0.006353833979554391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 23/100000 [00:27<33:07:49,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2200  | Reward:  -0.00014030000113186055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 24/100000 [00:28<32:45:42,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2300  | Reward:  0.0020726667543225414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 25/100000 [00:29<32:40:27,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2400  | Reward:  0.0005428215194869959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 26/100000 [00:31<33:06:10,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2500  | Reward:  -0.0007131938718626983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 27/100000 [00:32<33:08:26,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2600  | Reward:  0.005596607521172149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 28/100000 [00:33<33:04:48,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2700  | Reward:  0.00035802067418580616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 29/100000 [00:34<32:48:56,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2800  | Reward:  0.004266876858003821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 30/100000 [00:35<33:17:04,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2900  | Reward:  -0.0007093679626500257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 31/100000 [00:37<32:51:07,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3000  | Reward:  0.0021482889628098075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 32/100000 [00:38<33:04:52,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3100  | Reward:  0.0028515420790263908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 33/100000 [00:39<33:17:43,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3200  | Reward:  -0.0014226325634901884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 34/100000 [00:40<33:28:41,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3300  | Reward:  0.04590741531880613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 35/100000 [00:41<33:43:03,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3400  | Reward:  -0.0007744210606363567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 36/100000 [00:43<33:20:31,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3500  | Reward:  -0.0015030984991272792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 37/100000 [00:44<33:09:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3600  | Reward:  0.0006909045190034175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 38/100000 [00:45<32:56:07,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3700  | Reward:  0.0026857933180333255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 39/100000 [00:46<32:46:32,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3800  | Reward:  -0.001467917427218979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 40/100000 [00:47<32:40:58,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3900  | Reward:  0.0034386884460804274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 41/100000 [00:48<32:38:49,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4000  | Reward:  0.003587705212444127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 42/100000 [00:50<33:03:16,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4100  | Reward:  0.0006469715087158535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 43/100000 [00:51<33:00:13,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4200  | Reward:  0.0037150871506648805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 44/100000 [00:52<33:23:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4300  | Reward:  -0.0001787409235819892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 45/100000 [00:53<33:41:27,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4400  | Reward:  -0.00044084817387464144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 46/100000 [00:55<33:35:38,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4500  | Reward:  0.0008754560984879744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 47/100000 [00:56<33:45:53,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4600  | Reward:  0.010738684898885425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 48/100000 [00:57<33:32:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4700  | Reward:  -0.00016123318828058415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 49/100000 [00:58<33:27:17,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4800  | Reward:  -0.0028444657192274554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 50/100000 [00:59<33:25:27,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4900  | Reward:  0.0007290680691738691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 51/100000 [01:01<35:53:20,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5000  | Reward:  0.0006450614355751238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 52/100000 [01:02<37:56:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5100  | Reward:  -0.00011084809665956011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 53/100000 [01:04<39:17:51,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5200  | Reward:  -0.00021209339128908986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 54/100000 [01:05<40:31:49,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5300  | Reward:  -0.0009652146130507739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 55/100000 [01:07<41:29:41,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5400  | Reward:  9.01744163133095e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 56/100000 [01:09<42:07:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5500  | Reward:  -0.0005782648703960149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 57/100000 [01:10<42:06:35,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5600  | Reward:  0.001752135579558116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 58/100000 [01:12<43:08:41,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5700  | Reward:  0.0018419447032538392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 59/100000 [01:13<42:51:54,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5800  | Reward:  0.004902985992065539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 60/100000 [01:15<42:59:54,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5900  | Reward:  0.00293831386789977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 61/100000 [01:16<43:04:38,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6000  | Reward:  -0.0010746401158451236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 62/100000 [01:18<43:29:03,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6100  | Reward:  0.010595743617663916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 63/100000 [01:20<43:52:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6200  | Reward:  0.003369765946278637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 64/100000 [01:21<43:42:37,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6300  | Reward:  -0.00035246026171888503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 65/100000 [01:23<43:09:17,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6400  | Reward:  0.007342699569957426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 66/100000 [01:24<42:58:33,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6500  | Reward:  -0.0005650717671088308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 67/100000 [01:26<42:56:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6600  | Reward:  0.1704948971700899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 68/100000 [01:27<43:19:27,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6700  | Reward:  -0.0021089927217670086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 69/100000 [01:29<43:40:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6800  | Reward:  0.003001901302764208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 70/100000 [01:30<43:09:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6900  | Reward:  -0.0019178237414958208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 71/100000 [01:32<42:43:44,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7000  | Reward:  0.11883933253882809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 72/100000 [01:33<42:26:09,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7100  | Reward:  -0.00024860271705334606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 73/100000 [01:35<42:32:48,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7200  | Reward:  0.004454396586539276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 74/100000 [01:37<42:49:18,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7300  | Reward:  0.00021193650824355165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 75/100000 [01:38<43:06:02,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7400  | Reward:  -0.0004370398927821574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 76/100000 [01:40<43:26:13,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 7500  | Reward:  0.027177593898565142\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ea3eb1926dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# action = qt_opt.policy.act(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqt_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcem_optimal_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_action_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_value_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d3f4db30a085>\u001b[0m in \u001b[0;36mcem_optimal_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     74\u001b[0m             q_values = self.target_qnet1(\n\u001b[1;32m     75\u001b[0m                 \u001b[0mcuda_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             ).detach().cpu().numpy().reshape(-1) # 2 dim to 1 dim\n\u001b[1;32m     78\u001b[0m             \u001b[0mmax_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# select one maximal q\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9eea10688213>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import trange\n",
    "\n",
    "max_episodes  = int(1e7)\n",
    "batch_size=100\n",
    "max_steps = 200\n",
    "episode_rewards = []\n",
    "\n",
    "train_env = make_interf_env(123)\n",
    "test_env = make_interf_env(123)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(\n",
    "    'logs_v11'\n",
    ")\n",
    "\n",
    "\n",
    "for i_episode in trange(0, max_episodes, batch_size):\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_dist = 0\n",
    "    episode_angle = 0\n",
    "    episode_actions = np.zeros(ACTION_SPACE.shape[0])\n",
    "    \n",
    "    obs, action_target, value_target = train_env.reset()\n",
    "    state = obs.reshape(1, 16, 64, 64)\n",
    "\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # action = qt_opt.policy.act(state) \n",
    "        action = qt_opt.cem_optimal_action(state)\n",
    "        (obs, next_action_target, next_value_target), reward, done, info = train_env.step(action)\n",
    "        next_state = obs.reshape(1, 16, 64, 64)\n",
    "\n",
    "        episode_reward += reward\n",
    "        episode_actions += action\n",
    "        \n",
    "        # only last value needed\n",
    "        episode_dist = info['dist']\n",
    "        episode_angle = info['angle_between_beams']\n",
    "        #episode_reward = info['visib']\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done, action_target, [value_target])\n",
    "        state = next_state\n",
    "\n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = qt_opt.update(batch_size)\n",
    "        writer.add_scalar('loss', loss, i_episode)\n",
    "    \n",
    "        qt_opt.save_model('saved_model')\n",
    "        \n",
    "    writer.add_scalar('reward', episode_reward, i_episode)\n",
    "    writer.add_scalar('dist', episode_dist, i_episode)\n",
    "    writer.add_scalar('angle', episode_angle, i_episode)\n",
    "    \n",
    "    for i, a in enumerate(episode_actions):\n",
    "        writer.add_scalar('actions_{}'.format(i), a, i_episode)\n",
    "            \n",
    "    print('Episode: {}  | Reward:  {}'.format(i_episode, episode_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
